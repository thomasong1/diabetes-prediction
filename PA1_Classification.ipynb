{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 0. INITIALIZERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A HELPER FUNCTION TO DISPLAY SECTION TITLE\n",
    "def print_section(title):\n",
    "    print(f'{\"=\"*60}\\n{title}\\n{\"=\"*60}')\n",
    "\n",
    "### SEE ALL COLUMNS\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 1. DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# 2. INITIAL DATA INSPECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## GET OVERVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN DATASET\n",
    "print_section('TRAIN')\n",
    "train.info()\n",
    "display(train.head())\n",
    "display(train.describe())\n",
    "\n",
    "### TEST DATASET\n",
    "print_section('TEST')\n",
    "test.info()\n",
    "display(test.head())\n",
    "display(test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**REMARKS**:\n",
    "- 5 non-zero numeric features (`glucose_concentration`, `blood_pressure`, `skin_fold_thickness`, `serum_insulin`, `bmi`) have invalid zeros.\n",
    "- No column has any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## IDENTIFY ISSUES FOR DATA CLEANING\n",
    "A systematic check on potential data issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### A. CHECK DATA TYPES\n",
    "- Are all the columns in both datasets numeric (`int64` or `float64`)?\n",
    "- Only numeric columns are used in building ML models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CHECK DATA TYPES')\n",
    "\n",
    "def check_data_types(df, name):\n",
    "    if df.columns.equals(df.select_dtypes(include=['float64', 'int64']).columns):\n",
    "        print(f'✅ {name}: All data types are numeric (float64 or int64).')\n",
    "    else: \n",
    "        print(f'❌ {name}: There are non-numeric data types.')\n",
    "\n",
    "check_data_types(train, 'train')\n",
    "check_data_types(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### B. CHECK DUPLICATES\n",
    "- Are there any duplicated rows?\n",
    "- Are there any duplicated id?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CHECK ROW DUPLICATES')\n",
    "\n",
    "def check_row_dup(df, name):\n",
    "    row_dup = df.duplicated().sum()\n",
    "    if row_dup == 0:\n",
    "        print(f'✅ {name}: {row_dup} row duplicate found.')\n",
    "    else: \n",
    "        print(f'❌ {name}: {row_dup} row duplicates found.')\n",
    "\n",
    "check_row_dup(train, 'train')\n",
    "check_row_dup(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CHECK ID DUPLICATES')\n",
    "\n",
    "def check_id_dup(df, name):\n",
    "    total_entries = len(df)\n",
    "    unique_id = df['p_id'].nunique()\n",
    "    print(f\"Total entries: {total_entries}\")\n",
    "    print(f\"Unique p_id: {unique_id}\")\n",
    "    if total_entries == unique_id:\n",
    "        print(f'✅ {name}: No id duplicate found. 1 unique p_id per patient.\\n')\n",
    "    else: \n",
    "        print(f'❌ {name}: Multiple entries per patient detected!\\n')\n",
    "\n",
    "check_id_dup(train, 'train')\n",
    "check_id_dup(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### C. CHECK MISSING VALUES\n",
    "- Are there any columns with missing values?\n",
    "- If yes, how much do the missing values account for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CHECK MISSING VALUES')\n",
    "\n",
    "def check_missing(df, name):\n",
    "    ### COUNT OF MISSING VALUES\n",
    "    miss_cnt = df.isnull().sum()\n",
    "    \n",
    "    ### PERCENTAGE OF MISSING VALUES ROUNDED TO 2 DP\n",
    "    miss_pct = (miss_cnt / len(df) * 100).round(2)\n",
    "    \n",
    "    ### DF CONTAINING COLUMNS WITH MISSING VALUES\n",
    "    miss_df = pd.DataFrame({'missing_count': miss_cnt[miss_cnt>0],\n",
    "                               'missing_percentage': miss_pct[miss_pct>0]}) \\\n",
    "                    .sort_values('missing_count', ascending=False)\n",
    "\n",
    "    if len(miss_df) > 0:\n",
    "        print('\\nColumns with missing values:')\n",
    "        print(miss_df)\n",
    "        print(f\"\\n❌ {name}: Missing values detected.\")\n",
    "        print('Remove columns with % missing more than threshold in 3. DATA CLEANING.')\n",
    "    else:\n",
    "        print(miss_cnt)\n",
    "        print(f\"✅ {name}: No missing values detected.\\n\")\n",
    "\n",
    "check_missing(train, 'train')\n",
    "check_missing(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### D. CHECK INVALID VALUES\n",
    "- Are there any columns with invalid values, i.e. values that are unexpected based on domain understanding?\n",
    "- From the overview earlier, 5 numerical non-zero columns have invalid zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. DEFINE COLUMNS THAT CAN BE ZERO BUT NOT NEGATIVE\n",
    "test_zero_cols = ['no_times_pregnant', 'diabetes pedigree']\n",
    "train_zero_cols = test_zero_cols + ['diabetes']\n",
    "\n",
    "### 2. DEFINE COLUMNS THAT MUST BE POSITIVE (NOT ZERO/NEGATIVE)\n",
    "pos_cols = [col for col in train.columns if col not in train_zero_cols]\n",
    "\n",
    "### 3. UDF TO IDENTIFY INVALID (ZERO/NEGATIVE) VALUES\n",
    "def check_invalid(df, zero_cols, pos_cols, name):\n",
    "    print_section(f\"CHECK INVALID VALUES - {name}\")\n",
    "    \n",
    "    ### COLUMNS THAT CAN BE ZERO BUT NOT NEGATIVE\n",
    "    neg_only_cnt = (df[zero_cols] < 0).sum()\n",
    "    neg_only_pct = (neg_only_cnt / len(df) * 100).round(1)\n",
    "    zero_df = pd.DataFrame({\n",
    "        \"negatives_count\": neg_only_cnt,\n",
    "        \"negatives_percent\": neg_only_pct\n",
    "    })\n",
    "    print(\"1. COLUMNS THAT CAN BE ZERO BUT NOT NEGATIVE (CHECK NEGATIVE)\")\n",
    "    display(zero_df)\n",
    "    \n",
    "    ### COLUMNS THAT MUST BE POSITIVE (NOT ZERO/NEGATIVE)\n",
    "    zero_cnt = (df[pos_cols] == 0).sum()\n",
    "    zero_pct = (zero_cnt / len(df) * 100).round(1)\n",
    "    neg_cnt = (df[pos_cols] < 0).sum()\n",
    "    neg_pct = (neg_cnt / len(df) * 100).round(1)\n",
    "    pos_df = pd.DataFrame({\n",
    "        \"zeros_count\": zero_cnt,\n",
    "        \"zeros_percent\": zero_pct,\n",
    "        \"negatives_count\": neg_cnt,\n",
    "        \"negatives_percent\": neg_pct\n",
    "    })\n",
    "    print(\"2. COLUMNS THAT MUST BE POSITIVE (CHECK ZERO/NEGATIVE)\")\n",
    "    display(pos_df)\n",
    "\n",
    "    return zero_df, pos_df\n",
    "\n",
    "### 4. OUTPUT\n",
    "train_zero, train_pos = check_invalid(train, train_zero_cols, pos_cols, 'TRAIN')\n",
    "test_zero, test_pos = check_invalid(test, test_zero_cols, pos_cols, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "- ❌ Both train and test datasets have 5 non-zero columns with invalid zero values.\n",
    "- We shall treat the invalid values as \"missing values\".\n",
    "- In `3. DATA CLEANING`, the columns with \"missing values (invalid values)\" above a percentage threshold will be dropped, whereas the others below the threshold will be imputed using median.\n",
    "- Invalid value threshold: 30%\n",
    "- Columns to impute: `glucose_concentration`, `blood_pressure`, `bmi`\n",
    "- Columns to drop: `skin_fold_thickness`, `serum_insulin`\n",
    "- `skin_fold_thickness` is decided to be dropped.\n",
    "    - It has significant invalid percent (26% and 30.5%).\n",
    "    - It is a proxy for body fat, which is sufficiently represented by `bmi`.\n",
    "    - Imputing will likely introduce bias for marginal gain in model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### E. CHECK OUTLIERS\n",
    "- Methods: IQR & box plot.\n",
    "- TRAIN dataset is used for the bounds for both TRAIN dataset & TEST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE FEATURES (EXCLUDE ID & TARGET)\n",
    "feature_cols = [c for c in train.columns if c not in ['p_id', 'diabetes']]\n",
    "print(f\"Feature columns for outlier detection: {feature_cols}\")\n",
    "print(f\"\\nTotal: {len(feature_cols)}\")\n",
    "\n",
    "### UDF TO COMPUTE IQR BOUNDARIES\n",
    "# For each column in col, compute IQR-based lower and upper bounds.\n",
    "# Returns (lower, upper) where lower = Q1 - 1.5*IQR and upper = Q3 + 1.5*IQR.\n",
    "def compute_iqr_bounds(df, col):\n",
    "    s = df[col]\n",
    "    Q1 = s.quantile(0.25)\n",
    "    Q3 = s.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return lower, upper\n",
    "\n",
    "### UDF TO COUNT OUTLIERS BEFORE CAPPING\n",
    "def outlier_counts_by_bounds(df, cols, bounds_source='train'):\n",
    "    counts = {}\n",
    "    ### FOR EACH COLUMN IN COLS\n",
    "    for col in cols:\n",
    "        s = df[col]\n",
    "        ### COMPUTE IQR BOUNDS\n",
    "        lower, upper = compute_iqr_bounds(df, col)\n",
    "        ### COUNT OUTLIERS OUTSIDE THOSE BOUNDS\n",
    "        counts[col] = int(((s < lower) | (s > upper)).sum())\n",
    "    return pd.Series(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREP THE SUBPLOT GRID\n",
    "n_featureCols = len(feature_cols)\n",
    "ncols = 2  # TRAIN | TEST\n",
    "nrows = n_featureCols  # ONE ROW PER FEATURE\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(10, nrows*3))\n",
    "\n",
    "print_section('CHECK OUTLIERS (OUTSIDE IQR BOUNDS OF TRAIN)')\n",
    "\n",
    "### GET OUTLIER COUNTS\n",
    "before_train_counts = outlier_counts_by_bounds(train, feature_cols)\n",
    "before_test_counts = outlier_counts_by_bounds(test, feature_cols)\n",
    "\n",
    "### FOR EACH COLUMN IN TRAIN/TEST, PLOT THE BOXPLOT & SHOW OUTLIER COUNT\n",
    "for i, c in enumerate(feature_cols):\n",
    "    ### TRAIN BOXPLOT (LEFT COLUMN)\n",
    "    axes[i, 0].boxplot(train[c], vert=False)\n",
    "    axes[i, 0].set_title(f'\\nTrain - {c}')\n",
    "    ### RED IF OUTLIERS; GREEN IF NO OUTLIERS\n",
    "    train_color = 'green' if before_train_counts[c] == 0 else 'red'\n",
    "    axes[i, 0].text(1.05, 1, f\"Outliers: {before_train_counts[c]}\", \n",
    "                    transform=axes[i, 0].transAxes, fontsize=10, color=train_color, va='center')\n",
    "\n",
    "    axes[i, 1].boxplot(test[c], vert=False)\n",
    "    axes[i, 1].set_title(f'\\nTest - {c}')\n",
    "    ### RED IF OUTLIERS; GREEN IF NO OUTLIERS\n",
    "    test_color = 'green' if before_test_counts[c] == 0 else 'red'\n",
    "    axes[i, 1].text(1.05, 1, f\"Outliers: {before_test_counts[c]}\", \n",
    "                    transform=axes[i, 1].transAxes, fontsize=10, color=test_color, va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "- ❌ Both datasets have outliers, except `skin_fold_thickness` in train.\n",
    "- We shall ignore the outlier plots in `skin_fold_thickness` and `insulin_serum` which will be dropped later in `3. DATA CLEANING`.\n",
    "- For other outliers, they will be **intentionally left unmodified** during `3. DATA CLEANING` as they represent legitimate, extreme patient medical characteristics, rather than systematic data errors, due to the small percentage they account for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# 3. DATA CLEANING\n",
    "- From `2. INITIAL DATA INSPECTION`, there are no invalid data types (A), duplicates (B), or missing values (C).\n",
    "- Columns with invalid values (D) will be imputed or dropped.\n",
    "- Outliers (E) will be left unmodified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## D. HANDLE INVALID VALUES\n",
    "From `2. INITIAL DATA INSPECTION`,\n",
    "- Invalid value threshold: 30%\n",
    "- Columns to impute: `glucose_concentration`, `blood_pressure`, `bmi`\n",
    "- Columns to drop: `skin_fold_thickness`, `serum_insulin`\n",
    "- `skin_fold_thickness` is decided to be dropped.\n",
    "    - It has significant invalid percent (26% and 30.5%).\n",
    "    - It is a proxy for body fat, which is sufficiently represented by `bmi`.\n",
    "    - Imputing will likely introduce bias for marginal gain in model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('HANDLE INVALID VALUES')\n",
    "\n",
    "### DEFINE COLUMNS TO DROP/IMPUTE\n",
    "cols_drop = ['skin_fold_thickness', 'serum_insulin']\n",
    "cols_imp = ['glucose_concentration', 'blood_pressure', 'bmi']\n",
    "\n",
    "### DROP COLUMNS & CREATE COPIES FOR IMPUTATION\n",
    "print(f\"Dropping columns {cols_drop}...\\n\")\n",
    "train_imp = train.drop(cols_drop, axis=1).copy()\n",
    "test_imp = test.drop(cols_drop, axis=1).copy()\n",
    "\n",
    "### SANITY CHECK\n",
    "print(f\"Train: {train.shape} -> {train_imp.shape}\")\n",
    "print(f\"Test: {test.shape} -> {test_imp.shape}\")\n",
    "\n",
    "### IMPUTE ZEROS WITH TRAIN'S MEDIAN VALUES\n",
    "print(f\"\\nImputing zeros in {cols_imp}...\\n\")\n",
    "for col in cols_imp:\n",
    "    ### IMPUTE\n",
    "    median_val = train_imp[col].median()\n",
    "    train_imp[col] = train_imp[col].replace(0, median_val) \n",
    "    test_imp[col] = test_imp[col].replace(0, median_val)\n",
    "\n",
    "    ### SANITY CHECK\n",
    "    zeros_train = (train_imp[col] == 0).sum()\n",
    "    zeros_test = (test_imp[col] == 0).sum()\n",
    "    print(f\"{col}: train zeros = {zeros_train}, test zeros = {zeros_test}\")\n",
    "\n",
    "### LAST CHECK\n",
    "print(\"\\nColumn Stats\")\n",
    "display(train_imp.describe())\n",
    "display(test_imp.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## LATEST CLEANED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train_imp.copy()\n",
    "test_clean = test_imp.copy()\n",
    "feature_cols_clean = [c for c in train_clean.columns if c not in ['p_id', 'diabetes']]\n",
    "print(f\"Feature Columns After Cleaning: {feature_cols_clean}\\n\")\n",
    "\n",
    "print_section('TRAIN_CLEAN')\n",
    "train_clean.info()\n",
    "display(train_clean.head())\n",
    "print_section('TEST_CLEAN')\n",
    "test_clean.info()\n",
    "display(test_clean.head())\n",
    "\n",
    "print('''\n",
    "✅ DATA CLEANING COMPLETED!\n",
    "- TRAIN DATASET IS READY FOR EDA.\n",
    "- TEST DATASET IS READY FOR FEATURE ENGINEERING.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# 4. EDA\n",
    "- EDA is done only on the TRAIN dataset without touching the TEST dataset.\n",
    "- This is to prevent leaking information and artificially improving ML model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET PLOTTING STYLE\n",
    "# white background, visible grey gridlines\n",
    "# all figure sizes to 12 inches wide x 6 inches tall unless specified\n",
    "sns.set_theme(style='whitegrid', rc={'figure.figsize': (14, 6)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## TARGET VARIABLE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('TARGET VARIABLE ANALYSIS')\n",
    "\n",
    "### SHOW COUNT AND PERCENTAGE\n",
    "diabetes_count = train_clean['diabetes'].value_counts()\n",
    "diabetes_pct = train_clean['diabetes'].value_counts(normalize=True) * 100\n",
    "\n",
    "# :.1f and :.2f to round to 1 and 2 decimal place(s) respectively\n",
    "print('Diabetes Distribution:')\n",
    "print(f\"Non-Diabetic (0): {diabetes_count[0]} ({diabetes_pct[0]:.1f}%)\")\n",
    "print(f\"Diabetic (1): {diabetes_count[1]} ({diabetes_pct[1]:.1f}%)\")\n",
    "print(f\"\\nNon-Diabetic-to-Diabetic Ratio: {diabetes_count[0]/diabetes_count[1]:.2f}:1\")\n",
    "\n",
    "### VISUALIZE TARGET DISTRIBUTION\n",
    "# Create a figure of two subplots (1 row x 2 cols)\n",
    "# -------------------------------\n",
    "# |   Axes[0]   |   Axes[1]     |\n",
    "# -------------------------------\n",
    "fig, axes = plt.subplots(1, 2) \n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "# Bar Chart for Axes[0]\n",
    "diabetes_count.plot(kind='bar', ax=axes[0], color=colors, alpha=0.8)\n",
    "axes[0].set_title('Diabetes Distribution (Count)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Diabetes (0=No, 1=Yes)', fontsize=10)\n",
    "axes[0].set_ylabel('Count', fontsize=10)\n",
    "axes[0].set_xticklabels(['No Diabetes', 'Diabetes'], rotation=0)\n",
    "\n",
    "# Add count labels on bars (5 counts above current count y) \n",
    "# va='bottom': the bottom of text box sits above y-coordinate = y + 5\n",
    "for x, y in enumerate(diabetes_count): \n",
    "    axes[0].text(x, y + 5, str(y), ha='center', va='bottom')\n",
    "\n",
    "# Pie Chart for Axes[1]\n",
    "axes[1].pie(diabetes_count, labels=['No Diabetes', 'Diabetes'], \n",
    "            colors=colors, autopct='%1.0f%%', startangle=90,\n",
    "            explode=(0.05, 0.05), shadow=True)\n",
    "axes[1].set_title('Diabetes Distribution (Percentage)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "- The train dataset has class imbalance - 1.87 times more non-diabetic than diabetic patients.\n",
    "- This can make the ML models more biased towards the majority class (non-diabetic) and predict it more frequently.\n",
    "- Hence, the \"accuracy\" metric becomes unreliable, and cannot be the sole evaluation metric.\n",
    "- During model evaluation, **use other metrics (precision, recall, F1-score) alongside accuracy** to give a better picture of a model's performance.\n",
    "- https://www.geeksforgeeks.org/machine-learning/handling-imbalanced-data-for-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## STATISTICAL SUMMARY BY DIABETES STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('STATISTICAL SUMMARY BY DIABETES')\n",
    "\n",
    "### SUMMARY STATISTICS GROUPED BY DIABETES STATUS\n",
    "print('\\n--- Non-Diabetic (diabetes=0) ---')\n",
    "non_diabetic_stat = train_clean[train_clean['diabetes']==0][feature_cols_clean].describe()\n",
    "display(non_diabetic_stat.round(2))\n",
    "\n",
    "print('\\n--- Diabetic (diabetes=1) ---')\n",
    "diabetic_stat = train_clean[train_clean['diabetes']==1][feature_cols_clean].describe()\n",
    "display(diabetic_stat.round(2))\n",
    "\n",
    "### COMPARE MEDIANS BETWEEN GROUPS\n",
    "print('\\n--- Median Comparison: Diabetic vs Non-Diabetic ---')\n",
    "median_compare = pd.DataFrame({\n",
    "    'Non-Diabetic': train_clean[train_clean['diabetes']==0][feature_cols_clean].median(),\n",
    "    'Diabetic': train_clean[train_clean['diabetes']==1][feature_cols_clean].median(),\n",
    "})\n",
    "median_compare['Difference'] = median_compare['Diabetic'] - median_compare['Non-Diabetic']\n",
    "median_compare['Pct_Change'] = (median_compare['Difference'] / median_compare['Non-Diabetic'] * 100).round(0)\n",
    "display(median_compare.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "- **Median values are consistently higher in diabetic patients across all six numerical features**, indicating a systematic shift in central tendency rather than effects driven by outliers.\n",
    "- The most pronounced differences are observed in glucose concentration (+30%), age (+33%), number of pregnancies (median increase from 2 to 4), and diabetes pedigree score (+32%). BMI shows a moderate increase (+13%), while blood pressure exhibits only a small median difference (+3%).\n",
    "- These results are descriptive but do not imply statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## FEATURE DISTRIBUTIONS BY DIABETES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('FEATURE DISTRIBUTIONS BY DIABETES')\n",
    "\n",
    "### SET THE SUBPLOT GRID SHAPE\n",
    "n_cols = 2\n",
    "# Count no. of rows of subplots based on feature cols\n",
    "n_rows = int(np.ceil(len(feature_cols_clean) / n_cols))\n",
    "# Create the grid 18-inch wide with 4 inches per row\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows*4))\n",
    "# Flatten the 2D array of axes (n_rows, n_cols) to 1D for easy indexing of each subplot as axes(idx)\n",
    "axes = axes.ravel() \n",
    "\n",
    "### FOR EVERY COLUMN AND ITS ENUMERATED INDEX IN feature_cols_clean\n",
    "for idx, col in enumerate(feature_cols_clean):\n",
    "    ### PLOT FOR NON-DIABETIC\n",
    "    train_clean[train_clean['diabetes']==0][col].hist(\n",
    "        bins=20, alpha=0.6, label='No Diabetes', \n",
    "        color=colors[0], ax=axes[idx], edgecolor='black'\n",
    "    )\n",
    "    ### PLOT FOR DIABETIC\n",
    "    train_clean[train_clean['diabetes']==1][col].hist(\n",
    "        bins=20, alpha=0.6, label='Diabetes', \n",
    "        color=colors[1], ax=axes[idx], edgecolor='black'\n",
    "    )\n",
    "    ### SET PLOT ANNOTATIONS\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.2)\n",
    "\n",
    "### HIDE UNUSED SUBPLOT (IF ANY)\n",
    "for idx in range(len(feature_cols_clean), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "- Obvious difference in distributions of `glucose_concentration`, `bmi`, and `age` between diabetic and non-diabetic patients.\n",
    "- Diabetic patients tend to have **higher glucose concentration, BMI, and age**.\n",
    "- Other features (`no_times_pregnant`, `blood_pressure`, `diabetes pedigree`) have similar, overlapping distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CORRELATION ANALYSIS')\n",
    "\n",
    "### CALCULATE CORRELATION MATRIX\n",
    "corr_matrix = train_clean[feature_cols_clean + ['diabetes']].corr()\n",
    "\n",
    "### CREATE A FIGURE OF TWO SUBPLOTS (1 ROW X 2 COLS)\n",
    "fig, axes = plt.subplots(1, 2) \n",
    "\n",
    "### 1. CORRELATION HEATMAP IN LEFT SUBPLOT\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
    "axes[0].set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "### 2. HORIZONAL BAR PLOT IN RIGHT SUBPLOT\n",
    "diabetes_corr = corr_matrix['diabetes'].round(3).drop('diabetes').sort_values()\n",
    "# Dynamic color based on corr sign\n",
    "colorsCorr = [colors[0] if x > 0 else colors[1] for x in diabetes_corr.values]\n",
    "ax = diabetes_corr.plot(kind='barh', color=colorsCorr, ax=axes[1])\n",
    "\n",
    "# Add corr values as text labels at the end of each bar\n",
    "for i, (value, feature) in enumerate(zip(diabetes_corr.values, diabetes_corr.index)):\n",
    "    # x-position = end of bar (value)\n",
    "    # y-position = bar center (i)\n",
    "    ax.text(value + 0.01 if value > 0 else value - 0.01,  # offset outside the bar; dynamic based on corr sign\n",
    "            i, # y-coordinate of horizontal bar\n",
    "            f\"{value:.3f}\",  # formatted corr value to 3 dp\n",
    "            va='center',\n",
    "            ha='left' if value > 0 else 'right') # outside bar; dynamic based on corr sign\n",
    "\n",
    "axes[1].set_title('Feature Correlations with Diabetes', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "axes[1].set_ylabel('Features', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "- Glucose concentration has highest linear correlation with diabetes (0.502).\n",
    "- BMI is second most correlated with diabetes (0.297).\n",
    "- Other features show moderate-to-low correlation.\n",
    "- Some features are correlated with each other (multicollinearity), such as no_times_pregnant & age (0.53)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## BOX PLOTS BY DIABETES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('BOX PLOTS BY DIABETES STATUS')\n",
    "\n",
    "### BUILD BOX PLOTS FOR ALL FEATURES BY DIABETES STATUS\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(feature_cols_clean) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows*4))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_cols_clean):\n",
    "    train_clean.boxplot(column=col, by='diabetes', ax=axes[idx], patch_artist=True, return_type='dict')\n",
    "    axes[idx].set_title(f'{col} vs. diabetes status', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Diabetes (0=No, 1=Yes)', fontsize=10)\n",
    "    axes[idx].set_ylabel(col, fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "### HIDE UNUSED SUBPLOT\n",
    "for idx in range(len(feature_cols_clean), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('')  # Remove default parent title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "1. **Glucose concentration** shows the strongest separation between diabetic and non-diabetic groups.\n",
    "    - Diabetic individuals have much higher glucose (higher median, higher quartiles, more extreme values).\n",
    "    - This aligns with medical expectations and indicates glucose is the most discriminative feature.\n",
    "2. Several features shift upward for diabetics (**age**, **BMI**, **pregnancies**).\n",
    "    - **Age:** Diabetic individuals tend to be older on average.\n",
    "    - **BMI:** Higher BMI among diabetics suggests a connection with obesity-related risk.\n",
    "    - **Number of pregnancies:** The diabetic group shows higher median values & upper bound, but largely overlapping.\n",
    "    - These features show moderate separation but not as strong as **glucose concentration**.\n",
    "3. High overlap in **blood pressure** and **pedigree**.\n",
    "    - Although the distributions overlap, diabetics tend to have:\n",
    "        - Slightly higher blood pressure      \n",
    "        - Higher diabetes pedigree function (genetic risk indicator)\n",
    "    - These features contribute information but *individually* are not determining factors to separate between statuses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## SCATTER PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('SCATTER PLOTS')\n",
    "\n",
    "### CHOOSE MOST IMPORTANT FEATURES ONLY\n",
    "# glucose_concentration, bmi, age, diabetes pedigree\n",
    "\n",
    "### CREATE A FIGURE OF 2x2 SUBPLOTS\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10)) # Square plots for easier visualization\n",
    "axes = axes.ravel()\n",
    "\n",
    "### DEFINE THE SCATTER PLOT COMBINATIONS\n",
    "combinations = [\n",
    "    ('glucose_concentration', 'bmi'),\n",
    "    ('glucose_concentration', 'age'),\n",
    "    ('bmi', 'age'),\n",
    "    ('glucose_concentration', 'diabetes pedigree')\n",
    "]\n",
    "for idx, (feat1, feat2) in enumerate(combinations):\n",
    "    ### PLOT NON-DIABETIC\n",
    "    axes[idx].scatter(\n",
    "        train_clean[train_clean['diabetes']==0][feat1],\n",
    "        train_clean[train_clean['diabetes']==0][feat2],\n",
    "        alpha=0.5, c='#2ecc71', label='No Diabetes', s=30\n",
    "    )\n",
    "    ### PLOT DIABETIC\n",
    "    axes[idx].scatter(\n",
    "        train_clean[train_clean['diabetes']==1][feat1],\n",
    "        train_clean[train_clean['diabetes']==1][feat2],\n",
    "        alpha=0.5, c='#e74c3c', label='Diabetes', s=30\n",
    "    )\n",
    "    ### SET PLOT ANNOTATIONS\n",
    "    axes[idx].set_xlabel(feat1, fontsize=11)\n",
    "    axes[idx].set_ylabel(feat2, fontsize=11)\n",
    "    axes[idx].set_title(f'{feat1} vs. {feat2}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "1. **Glucose concentration is strongly associated with diabetes**\n",
    "    - Across all plots involving glucose concentration (vs BMI, age, and diabetes pedigree), individuals with diabetes generally have higher glucose levels compared to those without diabetes.\n",
    "    - This indicates that glucose concentration is a strong indicator of diabetes risk.\n",
    "2. **BMI and age show moderate separation**\n",
    "    - In the BMI vs age plot, individuals with diabetes tend to cluster at slightly higher BMI and older ages *compared to* those without diabetes, but there is considerable overlap.\n",
    "    - This suggests BMI and age contribute to diabetes risk but are **not as definitive alone**.\n",
    "3. **Diabetes pedigree contributes to risk prediction**\n",
    "    - In the glucose concentration vs diabetes pedigree plot, individuals with diabetes often have higher diabetes pedigree values, especially when combined with higher glucose levels.\n",
    "    - This shows that a family history of diabetes (captured by the diabetes pedigree) is an important factor when assessing diabetes risk.\n",
    "\n",
    "Overall, glucose concentration appears to be the strongest single indicator, while BMI, age, and diabetes pedigree provide additional context that can improve risk assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "# 5. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_before_FE = train_clean.copy()\n",
    "test_before_FE = test_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## CREATE CATEGORICAL FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### BMI CATEGORIES\n",
    "- WHO classifications: https://www.who.int/data/gho/data/themes/topics/topic-details/GHO/body-mass-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CREATE BMI CATEGORIES BASED ON WHO CLASSIFICATIONS')\n",
    "### DEFINE CATEGORICAL BINS AND LABELS BASED ON WHO CLASSIFICATIONS\n",
    "bins_bmi = [0, 18.5, 25, 30, float('inf')] # infinite upper bound\n",
    "labels_bmi = [0, 1, 2, 3]  # 0=Underweight, 1=Normal, 2=Overweight, 3=Obese\n",
    "\n",
    "### CUT WITH LEFT-INCLUSIVE (DEFAULT MODE IS RIGHT INCLUSIVE right=True)\n",
    "train_clean['bmi_category'] = pd.cut(train_clean['bmi'], bins=bins_bmi, labels=labels_bmi, right=False)\n",
    "test_clean['bmi_category'] = pd.cut(test_clean['bmi'], bins=bins_bmi, labels=labels_bmi, right=False)\n",
    "\n",
    "print('BMI CATEGORIES CREATED:')\n",
    "print('0 = Underweight (<18.5)')\n",
    "print('1 = Normal (18.5-24.9)')\n",
    "print('2 = Overweight (25-29.9)')\n",
    "print('3 = Obese (≥30)')\n",
    "\n",
    "### SHOW CATEGORIES FOR TRAIN DATA\n",
    "print('\\nDISTRIBUTION IN TRAIN DATA:')\n",
    "print(train_clean['bmi_category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### AGE GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CREATE AGE GROUPS')\n",
    "### DEFINE DESIRED CATEGORICAL BINS & LABELS\n",
    "bins_age = [20, 30, 40, 50, float('inf')] # infinite upper bound\n",
    "labels_age = [0, 1, 2, 3]  # 0=20-29, 1=30-39, 2=40-49, 3=50+\n",
    "\n",
    "### CUT WITH LEFT-INCLUSIVE (DEFAULT MODE IS RIGHT INCLUSIVE right=True)\n",
    "train_clean['age_group'] = pd.cut(train_clean['age'], bins=bins_age, labels=labels_age, right=False)\n",
    "test_clean['age_group'] = pd.cut(test_clean['age'], bins=bins_age, labels=labels_age, right=False)\n",
    "\n",
    "print('AGE GROUPS CREATED:')\n",
    "print('0 = 20-29 years')\n",
    "print('1 = 30-39 years')\n",
    "print('2 = 40-49 years')\n",
    "print('3 = 50+ years')\n",
    "\n",
    "### SHOW CATEGORIES FOR TRAIN DATA\n",
    "print('\\nDISTRIBUTION IN TRAIN DATA:')\n",
    "print(train_clean['age_group'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### GLUCOSE RISK CATEGORIES\n",
    "- Bins are based on https://diabetesjournals.org/care/article/29/suppl_1/s43/23313/Diagnosis-and-Classification-of-Diabetes-Mellitus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CREATE GLUCOSE RISK CATEGORIES')\n",
    "### DEFINE CATEGORICAL BINS & LABELS\n",
    "bins_glucose = [0, 100, 126, float('inf')]\n",
    "labels_glucose = [0, 1, 2]  # 0=Normal, 1=Prediabetic, 2=Diabetic\n",
    "\n",
    "### CUT WITH LEFT-INCLUSIVE (DEFAULT MODE IS RIGHT INCLUSIVE right=True)\n",
    "train_clean['glucose_category'] = pd.cut(train_clean['glucose_concentration'], bins=bins_glucose, labels=labels_glucose, right=False)\n",
    "test_clean['glucose_category'] = pd.cut(test_clean['glucose_concentration'], bins=bins_glucose, labels=labels_glucose, right=False)\n",
    "\n",
    "print('GLUCOSE RISK CATEGORIES CREATED:')\n",
    "print('0 = Normal (<100 mg/dL)')\n",
    "print('1 = Prediabetic (100-125.9 mg/dL)')\n",
    "print('2 = Diabetic range (≥126 mg/dL)')\n",
    "\n",
    "### SHOW CATEGORIES FOR TRAIN DATA\n",
    "print('\\nDISTRIBUTION IN TRAIN DATA:')\n",
    "print(train_clean['glucose_category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### CONVERT CATEGORICAL FEATURES TO NUMERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean['bmi_category'] = train_clean['bmi_category'].astype(int)\n",
    "test_clean['bmi_category'] = test_clean['bmi_category'].astype(int)\n",
    "\n",
    "train_clean['age_group'] = train_clean['age_group'].astype(int)\n",
    "test_clean['age_group'] = test_clean['age_group'].astype(int)\n",
    "\n",
    "train_clean['glucose_category'] = train_clean['glucose_category'].astype(int)\n",
    "test_clean['glucose_category'] = test_clean['glucose_category'].astype(int)\n",
    "\n",
    "print('✅ CATEGORICAL FEATURES (CONVERTED TO NUMERIC) CREATED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## CREATE RATIO & INTERACTION FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### GLUCOSE-BMI RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE\n",
    "train_clean['glucose_bmi_ratio'] = train_clean['glucose_concentration']/ (train_clean['bmi'] + 1e-6)\n",
    "test_clean['glucose_bmi_ratio'] = test_clean['glucose_concentration']/ (test_clean['bmi'] + 1e-6)\n",
    "\n",
    "### SHOW MEAN & RANGE FOR TRAIN DATA\n",
    "print_section('CREATE GLUCOSE-BMI RATIO')\n",
    "print(f\"Mean: {train_clean['glucose_bmi_ratio'].mean():.2f}\")\n",
    "print(f\"Range: {train_clean['glucose_bmi_ratio'].min():.2f} to {train_clean['glucose_bmi_ratio'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### BMI-AGE INTERACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE\n",
    "train_clean['bmi_age_interaction'] = train_clean['bmi'] * train_clean['age']\n",
    "test_clean['bmi_age_interaction'] = test_clean['bmi'] * test_clean['age']\n",
    "\n",
    "### SHOW MEAN FOR TRAIN DATA\n",
    "print_section('CREATE BMI-AGE INTERACTION')\n",
    "print(f\"Mean: {train_clean['bmi_age_interaction'].mean():.2f}\")\n",
    "print(f\"Range: {train_clean['bmi_age_interaction'].min():.2f} to {train_clean['bmi_age_interaction'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "### PREGNANCY-AGE INTERACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE\n",
    "train_clean['pregnancy_age_interaction'] = train_clean['no_times_pregnant'] * train_clean['age']\n",
    "test_clean['pregnancy_age_interaction'] = test_clean['no_times_pregnant'] * test_clean['age']\n",
    "\n",
    "### SHOW MEAN FOR TRAIN DATA\n",
    "print_section('CREATE PREGNANCY-AGE INTERACTION')\n",
    "print(f\"Mean: {train_clean['pregnancy_age_interaction'].mean():.2f}\")\n",
    "print(f\"Range: {train_clean['pregnancy_age_interaction'].min():.2f} to {train_clean['pregnancy_age_interaction'].max():.2f}\")\n",
    "\n",
    "print('\\n✅ Ratio and interaction features created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "## CREATE HIGH-RISK FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### HIGH GLUCOSE FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THRESHOLD\n",
    "glucose_threshold = 126  # mg/dL (diabetic range)\n",
    "\n",
    "### CREATE FLAGS\n",
    "train_clean['high_glucose_flag'] = (train_clean['glucose_concentration'] >= glucose_threshold).astype(int)\n",
    "test_clean['high_glucose_flag'] = (test_clean['glucose_concentration'] >= glucose_threshold).astype(int)\n",
    "\n",
    "### SHOW FLAG LOGIC & COUNT FOR TRAIN DATA \n",
    "print_section('CREATE HGH GLUCOSE FLAG')\n",
    "print(f'high_glucose_flag = 1 if glucose_concentration >= {glucose_threshold} mg/dL; 0 otherwise')\n",
    "print(f'{train_clean[\"high_glucose_flag\"].sum()} patients ({train_clean[\"high_glucose_flag\"].mean()*100:.1f}%) flagged in TRAIN data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### HIGH BMI FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE THRESHOLD\n",
    "bmi_threshold = 30  # Obese category\n",
    "\n",
    "### CREATE FLAGS\n",
    "train_clean['high_bmi_flag'] = (train_clean['bmi'] >= bmi_threshold).astype(int)\n",
    "test_clean['high_bmi_flag'] = (test_clean['bmi'] >= bmi_threshold).astype(int)\n",
    "\n",
    "### SHOW FLAG LOGIC & COUNT FOR TRAIN DATA\n",
    "print_section('CREATE HIGH BMI FLAG')\n",
    "print(f'high_bmi_flag = 1 if BMI >= {bmi_threshold}; 0 otherwise')\n",
    "print(f'{train_clean[\"high_bmi_flag\"].sum()} patients ({train_clean[\"high_bmi_flag\"].mean()*100:.1f}%) flagged in TRAIN data') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### HIGH BLOOD PRESSURE FLAG\n",
    "- Based on https://www.mayoclinic.org/diseases-conditions/high-blood-pressure/in-depth/blood-pressure/art-20050982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE THRESHOLD\n",
    "bp_threshold = 80  # mmHg (hypertension stage 1)\n",
    "\n",
    "### CREATE FLAGS\n",
    "train_clean['high_bp_flag'] = (train_clean['blood_pressure'] >= bp_threshold).astype(int)\n",
    "test_clean['high_bp_flag'] = (test_clean['blood_pressure'] >= bp_threshold).astype(int)\n",
    "\n",
    "### SHOW FLAG LOGIC & COUNT FOR TRAIN DATA\n",
    "print_section('CREATE HIGH BLOOD PRESSURE FLAG')\n",
    "print(f'high_bp_flag = 1 if BP >= {bp_threshold} mmHg; 0 otherwise')\n",
    "print(f'{train_clean[\"high_bp_flag\"].sum()} patients ({train_clean[\"high_bp_flag\"].mean()*100:.1f}%) flagged in TRAIN data') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### HIGH AGE FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE THRESHOLD\n",
    "age_threshold = 40\n",
    "\n",
    "### CREATE FLAGS\n",
    "train_clean['high_age_flag'] = (train_clean['age'] >= age_threshold).astype(int)\n",
    "test_clean['high_age_flag'] = (test_clean['age'] >= age_threshold).astype(int)\n",
    "\n",
    "### SHOW FLAG LOGIC & COUNT FOR TRAIN DATA\n",
    "print_section('CREATE HIGH AGE FLAG')\n",
    "print(f'high_age_flag = 1 if age >= {age_threshold}; 0 otherwise')\n",
    "print(f'{train_clean[\"high_age_flag\"].sum()} patients ({train_clean[\"high_age_flag\"].mean()*100:.1f}%) flagged in TRAIN data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### COMBINED RISK SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SUM THE HIGH-RISK FLAGS INTO RISK SCORES COLUMNS\n",
    "train_clean['risk_score'] = (train_clean['high_glucose_flag'] + \n",
    "                             train_clean['high_bmi_flag'] + \n",
    "                             train_clean['high_bp_flag'] + \n",
    "                             train_clean['high_age_flag'])\n",
    "\n",
    "test_clean['risk_score'] = (test_clean['high_glucose_flag'] + \n",
    "                            test_clean['high_bmi_flag'] + \n",
    "                            test_clean['high_bp_flag'] + \n",
    "                            test_clean['high_age_flag'])\n",
    "\n",
    "### SHOW COMBINED RISK SCORE FOR TRAIN DATA\n",
    "print_section('CREATE COMBINED RISK SCORE')\n",
    "print('risk_score: Sum of all high-risk flags (0-4)')\n",
    "print(f'Mean risk score for train: {train_clean[\"risk_score\"].mean():.2f}')\n",
    "\n",
    "print(f'\\nDISTRIBUTION IN TRAIN DATA:')\n",
    "print(train_clean['risk_score'].value_counts().sort_index())\n",
    "\n",
    "print('\\n✅ High-risk flags created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## CREATE POLYNOMIAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CREATE SQUARED FEATURES FOR NON-LINEAR RELATIONSHIPS')\n",
    "\n",
    "### GLUCOSE SQUARED\n",
    "train_clean['glucose_squared'] = train_clean['glucose_concentration'] ** 2\n",
    "test_clean['glucose_squared'] = test_clean['glucose_concentration'] ** 2\n",
    "print('glucose_squared: Glucose concentration squared')\n",
    "\n",
    "### BMI SQUARED\n",
    "train_clean['bmi_squared'] = train_clean['bmi'] ** 2\n",
    "test_clean['bmi_squared'] = test_clean['bmi'] ** 2\n",
    "print('bmi_squared: BMI squared')\n",
    "\n",
    "### AGE SQUARED\n",
    "train_clean['age_squared'] = train_clean['age'] ** 2\n",
    "test_clean['age_squared'] = test_clean['age'] ** 2\n",
    "print('age_squared: Age squared')\n",
    "\n",
    "print('\\n✅ Polynomial features created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## CREATE LOG-TRANSFORMED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('CREATE LOG-TRANSFORMED FEATURES FOR SKEWED FEATURES')\n",
    "\n",
    "### DIABETES PEDIGREE\n",
    "train_clean['pedigree_log'] = np.log1p(train_clean['diabetes pedigree'])\n",
    "test_clean['pedigree_log'] = np.log1p(test_clean['diabetes pedigree'])\n",
    "print('pedigree_log: Log-transformed diabetes pedigree function')\n",
    "\n",
    "print('\\n✅ Log-transformed features created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('FEATURE ENGINEERING SUMMARY')\n",
    "\n",
    "### FEATURE BEFORE AND AFTER FEATURE ENGINEERING\n",
    "original_features = list(train_before_FE.columns)\n",
    "new_features = [col for col in train_clean.columns if col not in original_features]\n",
    "\n",
    "print(f'\\n--- FEATURE COUNTS ---')\n",
    "print(f'Original feature count: {len(original_features)}')\n",
    "print(f'New features created: {len(new_features)}')\n",
    "print(f'Total feature count: {len(train_clean.columns)}')\n",
    "\n",
    "### NEW FEATURES\n",
    "print(f'\\n--- NEW FEATURES CREATED ---')\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f'{i:2d}. {feat}')\n",
    "\n",
    "### DISPLAY SAMPLE VALUES OF ENGINEERED FEATURES\n",
    "print(f'\\n--- SAMPLE VALUES OF ENGINEERED FEATURES ---')\n",
    "display(train_clean[new_features].head())\n",
    "\n",
    "print('\\n✅ FEATURE ENGINEERING COMPLETED!')\n",
    "print('Ready to proceed with model training.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISPLAY FINAL DATASETS READY FOR ML\n",
    "print_section('FINAL DATASETS READY FOR ML')\n",
    "print(f'Train shape: {train_clean.shape}')\n",
    "print(f'Test shape: {test_clean.shape}')\n",
    "print(f'\\nFirst 5 rows of TRAIN data:')\n",
    "display(train_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "# 6. MACHINE LEARNING\n",
    "- Goal: Train classification models and select the best one.\n",
    "- Algorithms: k-Nearest Neighbors, Decision Tree, Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE COPIES TO PRESERVE DATA\n",
    "train_before_ML = train_clean.copy()\n",
    "test_before_ML = test_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "## PREP DATA FOR MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('PREP DATA FOR MODELING')\n",
    "\n",
    "### DROP NON-PREDICTIVE IDENTIFIER (p_id) AND SEPARATE FEATURES (X) FROM TARGET (y)\n",
    "X = train_clean.drop(['p_id', 'diabetes'], axis=1)\n",
    "y = train_clean['diabetes']\n",
    "X_test_final = test_clean.drop(['p_id'], axis=1) # Reserve for final predictions\n",
    "\n",
    "### SPLIT TRAIN DATA INTO 80% TRAINING & 20% VALIDATION\n",
    "# Set the random_state seed (42) to ensure same random split for every run\n",
    "# stratify=y to have equal proportion of each class in train & val during split, based on y\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Train set: {X_train.shape}')\n",
    "print(f'Validation set: {X_val.shape}')\n",
    "\n",
    "### FEATURE SCALING\n",
    "scaler = StandardScaler()\n",
    "# FIT THE SCALER ON TRAIN SETS ONLY AND TRANSFORM IT\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# TRANSFORM THE VALIDATION AND TEST SETS USING THE SAME SCALER\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "print('\\n✅ Data prepared for modeling!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "## TRAIN CLASSIFICATION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZE MODELS IN A DICTIONARY\n",
    "models = {\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "### DEFINE AN EMPTY LIST TO STORE THE TRAINING RESULTS\n",
    "results = [] \n",
    "\n",
    "### LOOP THROUGH EACH MODEL\n",
    "for name, model in models.items():\n",
    "    ### SHOW WHAT MODEL IS BEING TRAINED\n",
    "    print(f'\\nTraining {name}...')\n",
    "    \n",
    "    ### TRAIN MODEL\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    ### USE TRAINED MODEL TO PREDICT THE TARGET (y) IN THE VALIDATION SET\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    ### CALCULATE METRICS\n",
    "    accuracy = accuracy_score(y_val, y_pred) # % of correct predictions overall\n",
    "    precision = precision_score(y_val, y_pred) # % of predicted positives that are actually positive\n",
    "    recall = recall_score(y_val, y_pred) # % of actual positives correctly predicted\n",
    "    f1 = f1_score(y_val, y_pred) # Harmonic mean of precision & recall (balances both)\n",
    "    \n",
    "    ### STORE RESULTS\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "    ### SHOW REPORT\n",
    "    print(f'☑️ {name} trained')\n",
    "    print(f'- Accuracy: {accuracy:.3f}')\n",
    "    print(f'- Precision: {precision:.3f}')\n",
    "    print(f'- Recall: {recall:.3f}')\n",
    "    print(f'- F1-Score: {f1:.3f}')\n",
    "\n",
    "print('\\n✅ All models trained successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "## COMPARE MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE COMPARISON DF FROM RESULTS, SORTED BY DESCENDING F1-Score\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print('--- MODEL PERFORMANCE METRICS COMPARISON ---')\n",
    "display(results_df)\n",
    "\n",
    "### VISUALIZE COMPARISON IN BAR CHART\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "### LEFT SUBPLOT axes[0] TO COMPARE ALL MODEL PERFORMANCE METRICS\n",
    "# Plot on the left\n",
    "ax1 = axes[0]\n",
    "results_df.plot(x='Model', y=['Accuracy', 'Precision', 'Recall', 'F1-Score'], \n",
    "                kind='bar', ax=ax1)\n",
    "\n",
    "# Set plot annotations\n",
    "ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "### RIGHT SUBPLOT axes[1] TO COMPARE F1-Score ONLY\n",
    "# Plot on the right\n",
    "ax2 = axes[1]\n",
    "results_df_sorted = results_df.sort_values('F1-Score')\n",
    "\n",
    "# Set plot annotations\n",
    "ax2.barh(results_df_sorted['Model'], results_df_sorted['F1-Score'], color='#3498db')\n",
    "ax2.set_title('F1-Score Ranking', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('F1-Score', fontsize=12)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add F1-Score value labels\n",
    "for i, v in enumerate(results_df_sorted['F1-Score']):\n",
    "    ax2.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_f1 = results_df.iloc[0]['F1-Score']\n",
    "\n",
    "print(f'\\n🏆 BEST MODEL: {best_model_name}')\n",
    "print(f'   F1-Score: {best_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "## EVALUATE BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET BEST MODEL AND ITS PREDICTIONS ON y IN THE VALIDATION SET\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_val_scaled)\n",
    "\n",
    "### CONFUSION MATRIX TO SEE WHERE THE MODEL IS MAKING MISTAKES\n",
    "print(f'--- CONFUSION MATRIX: {best_model_name} ---')\n",
    "cm = confusion_matrix(y_val, y_pred_best)\n",
    "\n",
    "### VISUALIZE CONFUSION MATRIX\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', # annot=True adds the values; fmt='d' present them as int\n",
    "            xticklabels=['No Diabetes', 'Diabetes'],\n",
    "            yticklabels=['No Diabetes', 'Diabetes'])\n",
    "\n",
    "# Set plot annotations\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### CLASSIFICATION REPORT\n",
    "# Precision: Of all patients predicted as diabetics, how many actually are?\n",
    "# Recall: Of all actual diabetics, how many were correctly identified?\n",
    "# F1-Score: Balances precision and recall - important when false negatives are costly.\n",
    "# Support: Number of true samples for each class, gives context to the metrics\n",
    "print(f'\\n--- Classification Report: {best_model_name} ---')\n",
    "print(classification_report(y_val, y_pred_best, target_names=['No Diabetes', 'Diabetes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "**CONFUSION MATRIX**\n",
    "- ✅ Most non-diabetic patients are correctly identified (68/80).\n",
    "- ⚠️ Some diabetic patients are missed (10/43).\n",
    "\n",
    "**CLASSIFICATION REPORT**\n",
    "- Accuracy: 0.81 => 82% of predictions are correct overall.\n",
    "- Macro avg (unweighted): 0.81 => average of F1 across classes (diabetics & non-diabetics), treats both classes equally.\n",
    "- Weighted avg: 0.82 => average of F1 weighted by class size, slightly favors the majority class (non-diabetics).\n",
    "- F1-Score for diabetic patients (0.75) is lower than for non-diabetic (0.86) => the model struggles more with detecting diabetics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "## MAKE PREDICTIONS ON TEST DATASET USING BEST MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE PREDICTIONS ON TEST DATASET USING BEST MODEL\n",
    "test_predictions = best_model.predict(X_test_scaled)\n",
    "\n",
    "### CREATE A DF OF PREDICTION RESULTS\n",
    "predictions_df = pd.DataFrame({\n",
    "    'p_id': test_clean['p_id'],\n",
    "    'predicted_diabetes': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"✅ Predictions completed for {len(predictions_df)} test samples\")\n",
    "print('Prediction distribution:')\n",
    "print(predictions_df['predicted_diabetes'].value_counts())\n",
    "\n",
    "### SHOW SAMPLE PREDICTIONS\n",
    "print('\\nSample predictions:')\n",
    "display(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "## ML SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section('ML RESULTS SUMMARY')\n",
    "print(f'''- ALGORITHMS TESTED: {', '.join(models)}\n",
    "- BEST MODEL: {best_model_name}                        \n",
    "- PERFORMANCE (VALIDATION SET):      \n",
    "    - Accuracy:  {results_df.iloc[0][\"Accuracy\"]:.4f}     \n",
    "    - Precision: {results_df.iloc[0][\"Precision\"]:.4f}     \n",
    "    - Recall:    {results_df.iloc[0][\"Recall\"]:.4f}      \n",
    "    - F1-Score:  {results_df.iloc[0][\"F1-Score\"]:.4f}\n",
    "- TEST PREDICTIONS:                  \n",
    "   - Total samples: {len(predictions_df)}\n",
    "   - Predictions saved in: predictions_df''')\n",
    "\n",
    "print('\\n✅ ML COMPLETED!')\n",
    "print('✅ PROJECT COMPLETED!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
